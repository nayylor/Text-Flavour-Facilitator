{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "empty = \"<empty>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    emojiVocab.add(empty)\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)\n",
    "vocabIdx = {word : i for i, word in enumerate(vocab)}\n",
    "eVocabIdx = {emoji : i for i, emoji in enumerate(emojiVocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=19720, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# Takes: dataset, word2vec model, and vocabulary from training\n",
    "# returns: list of tuples. First value is a torch of word embeddings for that sentence,\n",
    "# second value is the labels for each word\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '“', 'damnn', 'gurl', 'you', 'fine', 'a', 'fuck', '💦', '💦', '💦', 'did', 'you', 'fall', 'from', 'heaven', 'because', 'you', 'have', 'the', 'phattest', 'as', 'on', 'god', '🍑', '🍑', 'you', 'do', 'track', 'cool', 'because', 'i', 'can', 'track', 'dat', 'as', 'a', 'u', 'and', 'me', 'go', 'crazy', 'babey', '😎', 'hey', 'you', 'forgot', 'to', 'fill', 'out', 'this', 'survey', 'haha', 'just', 'kidding', 'that', '’', 's', 'my', 'phone', '😎', 'gim', 'me', 'ur', 'number', 'so', 'we', 'can', 'talk', 'all', 'night', '😉', 'what', '’', 's', 'that', 'you', '’', 're', 'single', 'haha', 'i', 'never', 'knew', 'but', 'what', 'a', 'coincidence', 'let', '’', 's', 'get', 'not', 'single', 'together', '😎', 'i', 'may', 'not', 'run', 'but', 'i', '’', 'm', 'boutta', 'run', 'up', 'on', 'dat', 'azz', '😤', 'haha', 'what', '’', 's', 'that', 'you', 'aren', '’', 't', 'looking', 'for', 'a', 'boyfriend', 'well', 'that', '’', 's', 'perfect', 'because', 'i', '’', 'm', 'no', 'boy', '😎', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "tensor([[-9.3177e-01, -2.9776e-01, -1.9192e+00,  ..., -4.1233e-01,\n",
      "          6.6682e-02,  1.1935e-01],\n",
      "        [-9.5044e-01, -2.1080e-01, -1.7289e+00,  ..., -4.2050e-01,\n",
      "          1.6278e-01, -7.8966e-02],\n",
      "        [-3.5400e-05,  1.2845e-02, -1.3731e-02,  ..., -1.0807e-02,\n",
      "          3.3908e-03, -2.3939e-03],\n",
      "        ...,\n",
      "        [-4.2613e-01, -9.1805e-02, -1.3128e+00,  ..., -1.4143e-01,\n",
      "          5.7582e-02,  1.6041e-01],\n",
      "        [-2.4548e+00, -1.4557e-02, -2.1166e+00,  ...,  4.2745e-01,\n",
      "         -1.1323e-01, -6.1282e-01],\n",
      "        [-1.3888e+00, -5.9507e-01, -1.8647e+00,  ...,  3.6282e-02,\n",
      "          7.8396e-01,  2.2058e-01]])\n",
      "['<s>', 'so', '😂', 'i', 'walked', '🚶\\u200d♂️', 'into', 'my', 'dad', 'bedroom', '🛌', 'and', 'i', 'saw', 'him', 'holding', 'a', 'real', 'lightsaber', '🍆', 'and', 'he', 'wa', 'stabbing', 'mommy', 'in', 'the', 'bum', 'bum', '😭', 'he', 'told', 'me', 'to', 'cum', 'here', 'so', 'i', 'did', 'i', 'wa', 'very', 'excited', '😛', 'to', 'hold', 'a', 'lightsaber', 'he', 'gave', 'it', 'to', 'me', 'and', 'it', 'vibrated', '😟', 'then', 'my', 'pee', 'pee', '🍆', '💦', 'started', 'to', 'grow', 'i', 'wa', 'so', 'scared', 'because', 'i', 'wa', 'a', 'girl', '👱\\u200d♀️', 'he', 'then', 'locked', 'the', 'door', 'and', 'he', 'stabbed', 'me', 'with', 'the', 'lightsaber', '</s>']\n",
      "[0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "tensor([[-9.3177e-01, -2.9776e-01, -1.9192e+00,  ..., -4.1233e-01,\n",
      "          6.6682e-02,  1.1935e-01],\n",
      "        [-1.4069e+00,  3.9938e-01, -2.9254e+00,  ..., -7.8487e-01,\n",
      "         -2.9841e-01,  8.9971e-02],\n",
      "        [-2.4760e+00, -7.0373e-01, -2.3739e+00,  ..., -1.4086e+00,\n",
      "          1.0933e+00, -2.1641e-01],\n",
      "        ...,\n",
      "        [-6.3424e-02, -4.0094e-01, -2.6267e+00,  ..., -1.3940e-02,\n",
      "         -7.6613e-01,  9.9822e-01],\n",
      "        [-1.7779e-02,  3.9336e-04, -3.8356e-02,  ..., -5.8678e-03,\n",
      "          1.5905e-02,  2.8388e-03],\n",
      "        [-1.3888e+00, -5.9507e-01, -1.8647e+00,  ...,  3.6282e-02,\n",
      "          7.8396e-01,  2.2058e-01]])\n",
      "['<s>', 'you', 'can', 'call', 'me', 'by', 'my', 'middle', 'name', '🥰', 'my', 'middle', 'name', 'is', '💕', 'vanessa', '💕', 'you', 'can', 'call', 'me', 'by', 'my', 'middle', 'name', 'if', 'you', 'want', 'too', '😋', 'or', 'you', 'can', 'call', 'me', 'shawdy', 'or', 'shorty', '😻', 'or', 'whatever', 'you', 'want', 'ok', '👌🏽', '🤤', '😯', 'thts', 'the', 'fuckin', 'gucci', 'gang', 'shyte', '💢', '❗', '️and', 'that', '’', 's', 'the', 'fuckin', 'period', 'shit', '⚡', '️', '😎', 'yk', 'the', 'fuckin', 'vibe', '😏', 'ahahaaa', 'ahaaaaa', '😛', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
      "tensor([[-9.3177e-01, -2.9776e-01, -1.9192e+00,  ..., -4.1233e-01,\n",
      "          6.6682e-02,  1.1935e-01],\n",
      "        [-1.0108e+00,  1.0922e+00, -3.0663e+00,  ..., -1.0381e+00,\n",
      "         -3.2698e-01,  4.8048e-01],\n",
      "        [-1.2786e+00,  7.5444e-01, -2.8062e+00,  ..., -9.7397e-01,\n",
      "         -3.2740e-02, -2.4887e-01],\n",
      "        ...,\n",
      "        [-1.1541e-02, -1.0776e-02, -3.2272e-02,  ...,  1.1258e-03,\n",
      "          1.0345e-02,  3.9498e-03],\n",
      "        [-8.2448e-01,  6.8891e-02, -1.0926e+00,  ..., -2.2499e-01,\n",
      "          4.2927e-01,  7.9648e-02],\n",
      "        [-1.3888e+00, -5.9507e-01, -1.8647e+00,  ...,  3.6282e-02,\n",
      "          7.8396e-01,  2.2058e-01]])\n",
      "['<s>', 'seriously', 'fuck', 'plastic', 'bag', 'everybody', 'is', 'like', 'oh', 'i', 'love', 'you', 'plastic', 'bag', 'and', 'thx', 'for', 'catching', 'my', 'cummies', '💦', '💦', 'plastic', 'bag', 'but', 'fuck', 'that', 'i', 'say', 'revolt', '✊', 'against', 'the', 'plastic', 'bag', 'oligarchy', 'theyve', 'kept', 'u', 'down', 'for', 'too', 'long', 'our', 'grocery', 'our', 'sex', 'toy', 'and', 'our', 'freedom', 'ha', 'been', 'coopted', 'by', 'the', 'plastic', 'bag', 'mafia', 'they', 'are', 'building', 'a', 'new', 'continent', 'in', 'the', 'pacific', 'while', 'you', 'carry', 'home', 'a', 'loaf', 'of', 'bread', 'in', 'one', 'of', 'their', 'bag', 'of', 'oppression', 'no', 'more', '😡', 'join', 'u', 'at', 'wwwfabricbagsforfreedom', 'and', 'get', 'a', '50', 'discount', 'on', 'totally', 'american', '🇺🇸', 'stuff', 'that', 'will', 'make', 'your', 'willy', 'bigger', 'and', 'alienate', 'friend', 'and', 'family', '☝', '️', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "tensor([[-0.9318, -0.2978, -1.9192,  ..., -0.4123,  0.0667,  0.1194],\n",
      "        [-0.0707, -0.0125, -0.1605,  ..., -0.0228,  0.0104,  0.0171],\n",
      "        [-0.5972, -0.0288, -2.2873,  ..., -0.3659,  0.1251,  0.5230],\n",
      "        ...,\n",
      "        [-1.2698, -0.1526, -1.5284,  ...,  0.5004,  0.1613, -0.0963],\n",
      "        [-2.1716, -0.9804, -2.3757,  ...,  0.8917,  0.7259, -0.4686],\n",
      "        [-1.3888, -0.5951, -1.8647,  ...,  0.0363,  0.7840,  0.2206]])\n",
      "['<s>', '🔫', 'sorry', 'that', 'i', 'believe', 'in', 'the', 'first', 'amendment', '🔫', '🇱🇷', 'sorry', 'for', 'trying', 'to', 'keep', 'our', 'traditional', 'american', 'value', '🇱🇷', '🤬', 'sorry', 'for', 'obliterating', 'you', 'in', 'every', 'debate', '🤬', '🏳️\\u200d🌈', 'sorry', 'for', 'trying', 'to', 'keep', 'men', 'men', 'and', 'woman', 'woman', '🏳️\\u200d🌈', '🤔', 'sorry', 'for', 'being', 'the', 'one', 'with', 'common', 'sense', '🤔', 'so', 'in', 'conclusion', '💣', '⚔', '️don', '’', 't', 'fuck', 'with', 'u', '⚔', '️', '💣', '😈', 'because', 'we', 'will', 'use', 'our', 'fact', 'over', 'feeling', '😈', '👱🏼\\u200d♂️', 'and', 'put', 'these', 'beta', 'cucks', 'in', 'their', 'place', '👱🏼\\u200d♂️', '✊🏻', '✊🏼', 'in', 'shapiro', 'we', 'trust', '✊🏻', '✊🏼', '</s>']\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "tensor([[-0.9318, -0.2978, -1.9192,  ..., -0.4123,  0.0667,  0.1194],\n",
      "        [-0.7111, -0.2542, -1.7698,  ..., -0.0212,  0.1816,  0.1709],\n",
      "        [-0.5301,  0.0271, -1.4213,  ..., -0.2850, -0.0570,  0.1801],\n",
      "        ...,\n",
      "        [-0.1275, -0.0270, -0.3328,  ..., -0.0441,  0.0767,  0.0323],\n",
      "        [-0.0990, -0.0902, -0.3055,  ..., -0.0307,  0.0415,  0.0426],\n",
      "        [-1.3888, -0.5951, -1.8647,  ...,  0.0363,  0.7840,  0.2206]])\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(5)\n",
    "for index,row in sample.iterrows():\n",
    "    print(row[\"words\"])\n",
    "    print(row[\"labels\"])\n",
    "    print(trainEmb[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building supervised models to predict next emoji\n",
    "\n",
    "+ RNN based architecture where we look at the hidden layer for every word\n",
    "  + Using hidden layer, predict if is an emoji and what emoji it is\n",
    "+ Asked TA from NLP class, they said this is similar to a language modelling problem where we only predict the set of emoji vocabulary\n",
    "  + Could also view as sequence labelling where tag is next emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward NN\n",
    "\n",
    "+ https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html\n",
    "+ Built using previous couple of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, eVocab_size, embedding_dim, context_size, hidden):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.linear2 = nn.Linear(hidden, eVocab_size)\n",
    "\n",
    "    def compute_loss(self, predicted_vector, label):\n",
    "        return self.loss(predicted_vector, label)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.activation(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFfeatures(data):\n",
    "    trigrams = []\n",
    "    for text in data[\"words\"]:\n",
    "        currTri = []\n",
    "        for i in range(len(text) - 2):\n",
    "            predictWord = text[i+2]\n",
    "            if not RE_EMOJI.match(predictWord):\n",
    "                predictWord = empty\n",
    "            currTri.append([[text[i], text[i+1]], predictWord])\n",
    "        trigrams.append(currTri)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_feats = FFfeatures(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1223 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started for epoch:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      " 10%|▉         | 121/1223 [01:51<25:03,  1.36s/it] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-33dd674a304e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meVocabIdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "losses = []\n",
    "model = NGramLanguageModeler(len(vocab), len(emojiVocab), EMBEDDING_DIM, CONTEXT_SIZE, 128)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0\n",
    "    print(\"Training started for epoch:{}\".format(epoch))\n",
    "    random.shuffle(train_feats)\n",
    "    start_time = time.time()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    minibatch_size = 10\n",
    "    N = len(train_feats)\n",
    "    for text in tqdm(train_feats):\n",
    "        for context, target in text:\n",
    "            context_idx = torch.tensor([vocabIdx[w] for w in context], dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            log_probs = model(context_idx)\n",
    "            loss = model.compute_loss(log_probs, torch.tensor([eVocabIdx[target]]))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            predicted_label = torch.argmax(log_probs)\n",
    "            correct += int(predicted_label == eVocabIdx[target])\n",
    "            total += 1\n",
    "    losses.append(total_loss)\n",
    "    print(\"Training completed for epoch:{}\".format(epoch))\n",
    "    print(\"Time for train:{}\".format(time.time() - start_time))\n",
    "    print(\"Accuracy:{}\".format(correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
