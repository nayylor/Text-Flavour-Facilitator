{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=28845, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '⚠', 'bruh', '⚠', '⚠', 'bruh', '⚠', '⚠', 'bruh', '⚠', 'ḇ̶̧̜̤̱̦͈͉̎̍͋͋͋r̶̥̹͚͕͚̠̖̣͚̀͋͆͒̾̂̊̊͐̀͘u̵̟̪͍̤̯̻̞̝͓͍̱͔͂̃̅̄̿̾̌͐̎̽̀̽̂̕ǔ̷̗̯͎̲̭̫͙͛͆̐͛͑̿̈́̂̚̚ͅu̴̼̼͍̻̲͇̬̾̐͌̀͌͒̀͒͆̈́͊̇ṵ̴̬̮̤̼̪̳̺͕̜̲̦̖̋̿̄̈̿͐̈́͠ư̶͕͈̻͚͖͙͕̫̜̑̃̇̋̽͘͜͝ų̸̺̘̈́̐͛ͅų̸̻͈̜͓̫͎̫͙̖̟̦̌̃̈͊̊̒̅ͅṵ̴͉͚͔͕͕̙̭͚̠̮̄̂̈͂̂́͌͘u̴̡̥̥̮͔͕̤̙͔̹͐͗͊̀̒ṵ̶̭̱͖͓̠͙͗̀̍̈́̍͜ų̷̠̮͓͈͔͍̯͔̖̳̘̿́͌̌̕ư̴̢̯͓̠̲̤͙̝͚̬̤͓͒̈̑͐̌̋͒͛͒̐̒̕͠ͅȗ̶̢̝͍̤̭̳̫̗̰͛̉̄̾͂̾̇̅ͅͅù̶̡̪̘͚͇̗̥̘͇͚̱̖̊̉̾ū̶͉͖u̸̢̢̺̰̝̻̱͇͉̘̾̈́̈́̑̈̒͘ͅu̷̢̡͔͉̞͊͐ư̸̮̩̞͙̪̝̮̻̤̤͋̎̽͋̍̎̏͋̅̐͆̈́̄u̵̢̧͇̞͓͎̻̺̫͙̘͎̞̗͊̕ư̵̠͝ȕ̸̧̬̼̱̱̩͔̣̝͔̟̜̞͇̕ư̵̘̩͇̠̻͙͖̗̳̜̿̈́̔̒̓h̷͍͓̪͇̹̭̆͂͑̀̅̄̂̂̾͊̓͊̓͝h̷̡̛̛̛̙͉̿͗̀̃͋͑̐͋͑̒͜͠ͅḩ̴̩̊͆̿̋̅̿̓̑͂̋̄͒̇͜h̸̢̫͕̦̖̉͛̌̄̓̽̕h̸̲͎̩̋̏̃̉̈́͛͝ḣ̸̢̡͔̘̮̳̯̯̅̐͐̕͠h̴͖̰͎̣̱̝̞̟̙̯̱͖͓̭̅̆͐̒͝͝ḩ̴̫̭̮̦̺̩͉̉ͅ', 'the', '👮', 'department', 'of', '🏠', 'homeland', '🗽', 'security', '🚔', 'ha', 'issued', 'a', '🅱', 'ruh', 'moment', '⚠', 'warning', '🚧', 'for', 'the', 'following', 'district', 'ligma', 'sugma', '🅱', 'ofa', 'and', 'sugondese', 'numerous', 'instance', 'of', '🅱', 'ruh', 'moment', '🅱', 'eing', 'triggered', 'by', '👀', 'cringe', '😬', 'normies', '🚽', 'have', '⏰', 'recently', '🕑', 'occurred', 'across', 'the', '🌎', 'continental', '🇺🇸', 'united', 'state', '🇺🇸', 'these', 'individual', 'are', '🅱', 'elieved', 'to', '🅱', 'e', 'highly', '🔫', 'dangerous', '🔪', 'and', 'should', '🚫', 'not', '❌', '🅱', 'e', 'approached', 'citizen', 'are', 'instructed', 'to', 'remain', 'inside', 'and', '🔒', 'lock', 'their', '🚪', 'door', 'under', '❌', 'no', '⛔', 'circumstance', 'should', 'any', 'citizen', '🙊', 'say', 'bruh', 'in', 'reaction', 'to', 'an', 'action', 'performed', '🅱', 'y', 'a', 'cringe', '😬', 'normie', '🚽', 'and', 'should', 'store', 'the', 'following', 'item', 'in', 'a', 'secure', '🔒', 'location', 'jahcoins', '💶', 'vbucks', '💴', 'gekyumes', 'foreskin', '🍆', 'poop', '💩', 'sock', 'juul', '💭', 'pod', 'ball', '🍒', 'crusher', 'and', 'dip', 'remain', 'tuned', 'for', 'further', 'instruction', '⚠', 'bruh', '⚠', '⚠', 'bruh', '⚠', '⚠', 'bruh', '⚠', '</s>']\n",
      "[1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n",
      "(tensor([[-0.6602,  0.9093, -0.5563,  ...,  0.2494, -0.5998, -0.1178],\n",
      "        [ 0.1864, -0.6238, -0.2801,  ...,  0.4462,  0.1219,  0.5782],\n",
      "        [ 0.1696,  0.0276, -0.3821,  ...,  0.0734, -0.0254,  0.0167],\n",
      "        ...,\n",
      "        [ 0.1696,  0.0276, -0.3821,  ...,  0.0734, -0.0254,  0.0167],\n",
      "        [ 0.1864, -0.6238, -0.2801,  ...,  0.4462,  0.1219,  0.5782],\n",
      "        [ 0.1688,  0.7186,  0.0021,  ..., -0.1241, -0.2070,  0.5638]]), [1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(train[\"words\"][0])\n",
    "print(train[\"labels\"][0])\n",
    "print(trainEmb[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
