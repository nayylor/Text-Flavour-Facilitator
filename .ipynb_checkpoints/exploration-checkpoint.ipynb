{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import emoji\n",
    "from gensim.models import Word2Vec\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<unk>\"\n",
    "wordEmbSize = 64\n",
    "data = pd.read_csv(\"data3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "+ Cleaning by using nltk word tokenizer and lemmatizer\n",
    "+ Adds spaces to emojis to separate them to different words using emoji library's re\n",
    "+ Add a start and end token\n",
    "+ Build vocab for words\n",
    "+ Build vocab for emojis\n",
    "+ Makes labels as 0 or 1 for each word. If label is 1, means that word is followed by an emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RE_EMOJI = emoji.get_emoji_regexp()\n",
    "tokenizer = nltk.word_tokenize\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "# tokens normalized\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#converting text to words\n",
    "def preprocessing(data, train=True):\n",
    "    newData = {\"words\":[], \"labels\":[]}\n",
    "    for text in data[\"texts\"]:\n",
    "        #converting to words\n",
    "        emoji_split = RE_EMOJI.split(text)\n",
    "        emoji_split = [x.strip() for x in emoji_split if x]\n",
    "        text = \" \".join(emoji_split)\n",
    "        textWords = LemNormalize(text)\n",
    "        textWords.insert(0,\"<s>\")\n",
    "        textWords.append(\"</s>\")\n",
    "        newData[\"words\"].append(textWords)\n",
    "        \n",
    "        #getting labels\n",
    "        labels = []\n",
    "        if train:\n",
    "            for i in range(1, len(textWords)):\n",
    "                word = textWords[i]\n",
    "                if RE_EMOJI.match(word):\n",
    "                    labels.append(1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            labels = [0 * len(textWords)]\n",
    "        newData[\"labels\"].append(labels)\n",
    "    return pd.DataFrame(newData)\n",
    "\n",
    "def make_vocabs(data):\n",
    "    vocab = set()\n",
    "    vocab.add(UNK)\n",
    "    emojiVocab = set()\n",
    "    for text in data[\"words\"]:\n",
    "        for word in text:\n",
    "            vocab.add(word)\n",
    "            if RE_EMOJI.match(word):\n",
    "                emojiVocab.add(word)\n",
    "    return vocab, emojiVocab\n",
    "        \n",
    "train = preprocessing(data, True)\n",
    "vocab, emojiVocab = make_vocabs(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "+ Using gensim's Word2Vec\n",
    "+ Builds model with word embedding size specified earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=28845, size=64, alpha=0.025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericsun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def getEmbModel(data, vocab):\n",
    "    docs = [[UNK]]\n",
    "    docs.extend(data[\"words\"])\n",
    "    model = Word2Vec(docs, min_count = 1, size = wordEmbSize)\n",
    "    print(model)\n",
    "    return model\n",
    "\n",
    "# Takes: dataset, word2vec model, and vocabulary from training\n",
    "# returns: list of tuples. First value is a torch of word embeddings for that sentence,\n",
    "# second value is the labels for each word\n",
    "def getEmb(data, model, vocab):\n",
    "    vecData = []\n",
    "    for text,y in zip(data[\"words\"],data[\"labels\"]):\n",
    "        wordEmb = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                wordEmb.append(model[word])\n",
    "            else:\n",
    "                wordEmb.append(model[UNK])\n",
    "        wordEmb = torch.FloatTensor(wordEmb)\n",
    "        vecData.append((wordEmb, y))\n",
    "    return vecData\n",
    "\n",
    "model = getEmbModel(train, vocab)\n",
    "trainEmb = getEmb(train, model, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lol', '😂', 'if', 'i', 'ever', 'had', 'any', 'homosexual', '🤢', 'child', '👶', 'know', 'what', 'i', '’', 'd', 'do', '🤔', 'nothing', 'i', '’', 'd', 'respect', '✊', 'their', 'choice', 'to', 'become', 'homosexual', '🤢', 'just', 'like', 'i', 'respected', '✊', 'my', 'retard', 'cousin', 'bilo', '’', 's', 'sex', 'change', 'to', 'a', 'gay', 'tranny', 'alpaca', '🦙', 'and', 'why', '🤔', 'because', '💁\\u200d♀️', 'i', '’', 'm', 'not', 'a', 'homophobic', 'old', 'white', 'male', '😂', 'or', 'the', 'average', 'reddit', 'user', '🤷\\u200d♂️', 'i', 'am', 'chad', '😎', 'i', 'respect', '✊', 'people', '’', 's', 'choice', 'a', 'individual', 'and', 'sex', 'the', 'woman', '👩\\u200d🦳', '</s>']\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-0.9994,  0.0748, -0.3478,  ...,  0.7569,  0.0692, -0.1553],\n",
      "        [-2.2077,  0.5740, -1.6367,  ...,  2.1574, -0.5370, -0.5692],\n",
      "        ...,\n",
      "        [-0.5951, -0.1858, -0.5898,  ...,  0.6699,  0.0998,  0.0808],\n",
      "        [-0.0093,  0.0291, -0.3190,  ...,  0.1398, -0.0721, -0.0136],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'you', 'stupid', '🤡', 'as', 'black', '👸🏾', 'bitch', '🐶', 'stop', '🙅\\u200d♀️', 'screenshotting', '🖥', 'my', 'story', '📖', 'or', 'say', 'goodbye', '🙋🏻\\u200d♀️', 'to', 'you', 'last', 'day', '👿', 'this', 'is', '🧖\\u200d♂️', 'the', 'end', '❌', 'your', 'time', '⏲', 'ha', 'come', '⌛', '️', 'it', 'a', 'wrap', '🎁', 'the', 'clock', '⏰', 'ha', 'struck', '🕛', 'midnight', '🌌', 'time', '🔔', 'to', 'go', '🔚', 'time', '🕘', 'to', 'pack', '🧰', 'it', 'all', 'in', '📲', 'youve', 'had', 'a', 'good', 'run', '🏃\\u200d♂️', 'youve', 'lived', '🍎', 'quite', 'a', 'life', '🎫', 'youve', 'done', '🏁', 'this', '🏄🏾\\u200d♀️', 'and', 'that', '🤹🏼\\u200d♂️', 'youve', 'loved', '❤', '️', 'lost', '💔', 'and', 'loved', '💕', 'some', 'more', '💞', 'youve', 'accumulated', '📸', 'quite', 'a', 'collection', '🛒', 'of', 'memory', '🗝', 'about', 'people', '🤱', 'place', '🏞', 'and', 'thing', '🧸', 'take', '🕞', 'a', 'moment', 'to', 'reflect', '🔮', 'on', 'them', '💎', 'all', 'allow', 'the', '🖼', 'entire', 'tableau', '🎦', 'of', 'your', 'time', '🕦', 'here', 'on', 'earth', '🌎', 'to', 'flash', '🎇', 'before', 'your', 'eye', '👀', 'aquire', 'a', 'final', '💤', 'overwhelming', '‼', '️', 'sense', '☺', '️', 'of', 'peace', '☮', '️', 'know', '🧠', 'that', 'everything', '🌏', 'will', 'be', 'okay', '👍', 'look', 'into', '👁', 'that', 'approaching', 'tunnel', '🚊', 'of', 'light', '☀', '️', 'reach', 'out', '🕺', 'your', 'hand', '👋🏽', 'to', 'jesus', '⛪', '️', 'be', 'struck', '💥', 'by', 'the', '🌟', 'presence', 'of', '🙌', 'god', '⚡', '️', 'face', '👤', 'your', 'own', '☠', '️', 'mortality', '🥀', 'take', 'it', '😤', 'like', 'a', '🧔🏼', 'man', 'you', '👈', 'have', 'little', '👌', 'choice', 'now', 'might', 'a', 'well', '🤷\\u200d♀️', 'accept', '😌', 'the', 'way', 'thing', '💅🏿', 'are', 'you', 'dead', '👻', '</s>']\n",
      "[0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-1.5767, -1.1055, -0.6313,  ...,  1.8308,  0.5306, -0.1961],\n",
      "        [-0.9561, -0.2979, -0.3954,  ...,  0.8991,  0.1773, -0.2182],\n",
      "        ...,\n",
      "        [-0.7522, -0.1611, -0.2687,  ...,  0.7636, -0.0055,  0.0373],\n",
      "        [-0.6412,  0.9150, -0.6565,  ...,  0.3183, -0.4655, -0.2694],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'this', 'world', '🌎', 'is', 'rotten', '🤢', '😤', 'and', 'those', 'who', 'are', 'making', 'it', 'rot', '😱', '😷', '😰', 'deserve', 'to', 'die', '😡', '💀', '🔪', 'someone', 'ha', 'to', 'do', 'it', 'so', 'why', 'not', 'me', '🤔', '🤔', 'even', 'if', 'it', 'mean', 'sacrificing', '🔪', 'my', 'own', 'mind', '🧠', '😱', 'and', 'soul', '👻', 'it', 'worth', 'it', '👍', '🙌', 'because', 'the', 'world', '😞', 'cant', 'go', 'on', 'like', 'this', '😔', '😤', '😣', 'i', 'wonder', '🤔', '👀', 'what', 'if', 'someone', 'else', '🙇\\u200d♂️', '👨', 'had', 'picked', '🤚', 'up', 'this', 'notebook', '📔', '✏', 'is', 'there', 'anyone', '😰', 'out', 'there', 'other', 'than', 'me', '🙅\\u200d♂️', 'whod', 'be', 'willing', 'to', 'eliminate', 'the', 'vermin', '😒', '🐁', '🐀', 'from', 'the', 'world', '🤔', '🤷\\u200d♂️', 'if', 'i', 'dont', 'do', 'it', '👊', 'then', 'who', 'will', '😧', 'thats', 'just', 'it', 'there', 'no', 'one', '🙄', '🚫', '🙅\\u200d♂️', 'but', 'i', 'can', 'do', 'it', '☝', '️', '🤞', 'in', 'fact', 'im', 'the', 'only', 'one', 'who', 'can', '😋', '😜', 'ill', 'do', 'it', '😉', '😙', 'using', 'the', 'death', 'note', '☠', '️', '📔', 'ill', 'change', 'the', 'world', '😋', '😫', '👌🏻', '🤘🏻', '</s>']\n",
      "[0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-2.4187, -1.6323,  0.1832,  ...,  1.5042,  0.2358,  0.7519],\n",
      "        [-1.1427, -0.4600, -0.2972,  ...,  1.3303, -0.3635,  0.0958],\n",
      "        ...,\n",
      "        [-0.0134,  0.1986, -0.3422,  ...,  0.2392, -0.1800, -0.1248],\n",
      "        [ 0.1746,  0.1771, -0.3954,  ...,  0.1045, -0.1188, -0.0163],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'story', 'time', '🕛', '🕛', 'sister', '💁\\u200d♀️', '💁\\u200d♀️', 'so', 'basically', 'i', 'wa', 'in', 'class', 'listening', 'to', 'billie', 'eilish', '❤', '️', '❤', '️and', 'my', 'headphone', 'got', 'unplugged', '🔌', '🔌', '🔌', '😩', '😩', 'and', 'it', 'played', 'bad', 'guy', '🥺', '🥺', 'out', 'loud', '🔊', '🔊', 'so', 'anyway', 'it', 'wa', 'playing', 'out', 'loud', 'and', 'all', 'the', 'girl', '👭', '👭', 'were', 'completely', 'vibing', 'to', 'it', '💏', '💏', 'and', 'they', 'were', 'like', 'slayyyyy', '🔪', '🔪', 'sisterrr', '☠', '️', '☠', '️and', 'i', 'wa', 'gon', 'na', 'say', 'something', 'back', 'when', 'a', 'boy', '🤮', '🤮', 'approached', 'me', 'and', 'said', '🗣', '️', '🗣', '️', 'uh', 'billie', '🤮', '🤮', 'eilish', 'is', 'so', 'cringe', 'why', 'dont', 'you', '🤢', '🤢', 'listen', 'to', 'kanye', '🧐', '🧐', 'and', 'i', 'wa', 'shook', '😲', '😲', '😳', '😳', 'and', 'completely', 'flipped', 'the', 'f', 'out', '🤬', '🤬', 'i', 'said', 'you', 'dumb', '🤬', '🤬', 'ignorant', '👨🏻', '\\u200d', '🦰', '👨🏻', '\\u200d', '🦰', 'male', 'billie', 'literally', '🙏🏻', '🙏🏻', '🙏🏻', 'saved', 'my', 'life', '🙏', '🙏', 'i', 'wa', 'cutting', 'myself', '😫', '😫', 'for', 'my', 'depression', 'since', 'daddy', '🧔🏻', '🧔🏻', 'didnt', 'get', 'me', 'ticket', 'to', 'coachella', '👴', '👴', 'and', 'a', 'pentagram', '⛧', 'formed', 'on', 'the', 'ground', 'and', 'billie', 'rose', '👆🏻', '👆🏻', '👆🏻', '👆🏻', 'up', 'from', 'it', '🧖\\u200d♀️', '🧖\\u200d♀️', 'and', 'she', 'said', 'put', 'your', 'faith', 'in', 'allah', 'for', 'he', 'is', 'the', 'most', 'merciful', '💣', '💣', 'and', 'then', 'she', 'left', 'and', 'i', 'wa', 'so', 'inspired', 'that', 'i', 'read', 'the', 'korean', 'promised', 'to', 'slay', '🔪', '🔪', '🔪', 'every', 'infidel', 'in', 'my', 'path', '🧕', '🧕', 'until', 'shariah', 'law', 'wa', 'implemented', 'world', 'wide', 'he', 'then', 'wa', 'like', 'i', 'wont', 'allow', 'a', 'mujahid', 'to', 'spread', 'the', 'gentle', 'message', 'of', 'mohamabamba', 'and', 'then', 'summoned', 'a', 'djinn', '🧞\\u200d♂️', '🧞\\u200d♂️', 'in', 'the', 'shape', 'of', 'jahsehs', 'foreskin', 'and', 'he', 'said', 'he', 'wa', 'the', 'servant', 'of', 'shaytan', '👹', '👹', 'well', 'i', 'wasnt', 'going', 'to', 'let', 'blasphemy', '😡', '😡', 'go', 'unpunished', 'and', 'chanted', 'oh', 'allah', 'the', 'most', 'kind', 'and', 'beautiful', 'please', 'banish', 'these', 'heathen', 'back', 'to', 'hell', '🔙', '🔙', 'and', 'suddenly', 'the', 'heaven', 'opened', 'and', 'we', 'loooked', 'and', 'it', 'wa', 'billie', '🤩', '🤩', 'she', 'said', 'i', 'am', 'allah', '🙇🏻', '\\u200d', '♂', '</s>']\n",
      "[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-0.7427, -0.1527, -0.3703,  ...,  0.7159, -0.0371, -0.0536],\n",
      "        [-1.5682, -0.6376, -0.5195,  ...,  1.8709, -0.5231,  0.1583],\n",
      "        ...,\n",
      "        [-0.1862, -0.0678, -5.3837,  ...,  1.0025, -0.3832, -0.1431],\n",
      "        [-0.2198, -0.1540, -1.4406,  ...,  0.3978, -0.2948,  0.0774],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n",
      "['<s>', 'hello', 'i', 'am', 'xxbedwarsplayzyt', 'i', 'have', 'been', 'playing', 'on', 'your', 'server', 'for', 'over', '2', 'year', 'now', 'yesterday', 'i', 'got', 'banned', '🔨', 'for', 'using', 'a', 'hacked', 'client', '💻', 'this', 'wa', 'a', 'complete', 'accident', 'i', 'swear', '😭', 'i', 'wa', 'playing', 'on', 'my', 'survival', 'world', 'and', 'i', 'needed', 'diamond', 'so', 'i', 'installed', 'a', 'hack', 'client', 'and', 'turn', 'ed', 'on', 'auto', 'mine', 'and', 'other', 'hack', 'so', 'i', 'get', 'diamond', '😂', 'i', 'got', 'bored', 'so', 'i', 'went', 'onto', 'your', 'server', '👌', 'i', 'didnt', 'know', 'that', 'i', 'had', 'hack', '💻', 'on', 'because', 'i', 'just', 'opened', 'up', 'my', 'inventory', 'to', 'go', 'into', 'game', '7', 'second', 'later', 'i', 'wa', 'banned', 'for', 'using', 'hack', '😳', '😱', 'i', 'am', 'extremely', 'sorry', 'for', 'this', 'i', 'hope', 'you', 'understand', 'please', 'unban', '🙏', 'me', 'im', 'literally', 'shaking', 'and', 'cry', 'right', 'now', '😭', '😭', '</s>']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "tensor([[-1.6632,  0.3112, -0.3850,  ...,  1.0191, -0.0796, -0.1400],\n",
      "        [-0.2904,  0.0321, -0.2804,  ...,  0.3336,  0.0031, -0.0874],\n",
      "        [-3.0416, -1.1281, -0.6507,  ...,  1.9127,  1.8111,  0.4478],\n",
      "        ...,\n",
      "        [-0.7745, -0.5737, -0.8827,  ...,  3.1166,  0.5821,  1.3847],\n",
      "        [-0.7745, -0.5737, -0.8827,  ...,  3.1166,  0.5821,  1.3847],\n",
      "        [-0.8733,  0.3310, -0.4389,  ...,  0.4127, -0.5094, -0.1236]])\n"
     ]
    }
   ],
   "source": [
    "sample = train.sample(5)\n",
    "for index,row in sample.iterrows():\n",
    "    print(row[\"words\"])\n",
    "    print(row[\"labels\"])\n",
    "    print(trainEmb[index][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building supervised models to predict next emoji\n",
    "\n",
    "+ RNN based architecture where we look at the hidden layer for every word\n",
    "  + Using hidden layer, predict if is an emoji and what emoji it is\n",
    "+ Asked TA from NLP class, they said this is similar to a language modelling problem where we only predict the set of emoji vocabulary\n",
    "  + Could also view as sequence labelling where tag is next emoji or no emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
